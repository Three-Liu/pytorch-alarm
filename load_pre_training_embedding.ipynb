{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. explain\n",
    "- What is `nn.Embedding` ?\n",
    "    - `nn.Embedding()` will randomly initialize embedding vector for each word in vocabulary, it will automatically update the weight as training start\n",
    "    - using pre-training embedding from others will reduce your time cost on embedding training part\n",
    "- How it works?\n",
    "    - by calling `emb.load_state_dict` will adopt the designated pre-training embedding.\n",
    "    \n",
    "### 2. fake some data \n",
    "> 1) the size of vocabulary set to 10 <br/>\n",
    "> 2) the dimension of embedding layer set to 5 <br/>\n",
    "> 3) fake a pre-training embedding<br/> \n",
    "> **Remember to set dtype to float!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0568, 0.5747, 0.1129, 0.8728, 0.5366],\n",
      "        [0.1203, 0.3308, 0.1651, 0.9735, 0.7763],\n",
      "        [0.3559, 0.7236, 0.1384, 0.3992, 0.3596],\n",
      "        [0.5445, 0.9686, 0.8894, 0.5363, 0.0135],\n",
      "        [0.1305, 0.4640, 0.9225, 0.5158, 0.2404],\n",
      "        [0.6249, 0.7514, 0.5071, 0.6078, 0.1853],\n",
      "        [0.6547, 0.9130, 0.7297, 0.1336, 0.6045],\n",
      "        [0.7153, 0.9287, 0.1015, 0.7253, 0.5171],\n",
      "        [0.5384, 0.3728, 0.9658, 0.2157, 0.7109],\n",
      "        [0.7533, 0.0976, 0.9411, 0.6239, 0.1107]])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "embed_dim = 5\n",
    "fake_pre_embedding = torch.rand((vocab_size,embed_dim),dtype=torch.float32)\n",
    "print(fake_pre_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. create a layer\n",
    "> 1) create an embedding layer <br/>\n",
    "> 2) calling `load_state_dict` to load pretraining embedding <br/>\n",
    "> 3) using `weight.requires_grad` to disable the weight update <br/> \n",
    "> **Remember to set dtype to float!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, embedding_pretrained, freeze):\n",
    "        super(FC,self).__init__()\n",
    "        self.emb_layer = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.emb_layer.load_state_dict({'weight':embedding_pretrained})\n",
    "        if freeze:\n",
    "            ''' if you enable this line, the weight in embedding layer will not update while the model training '''\n",
    "            emb_layer.weight.requires_grad = False\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        embedded_inputs = self.emb_layer(inputs)\n",
    "        print(embedded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. fake a input sequence\n",
    "> 1) form an input sequence <br/>\n",
    "> **Remember to set dtype to Long!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqence:tensor([3, 4, 0, 8, 9, 8, 0, 5, 2, 3])\n",
      "tensor([[0.5445, 0.9686, 0.8894, 0.5363, 0.0135],\n",
      "        [0.1305, 0.4640, 0.9225, 0.5158, 0.2404],\n",
      "        [0.0568, 0.5747, 0.1129, 0.8728, 0.5366],\n",
      "        [0.5384, 0.3728, 0.9658, 0.2157, 0.7109],\n",
      "        [0.7533, 0.0976, 0.9411, 0.6239, 0.1107],\n",
      "        [0.5384, 0.3728, 0.9658, 0.2157, 0.7109],\n",
      "        [0.0568, 0.5747, 0.1129, 0.8728, 0.5366],\n",
      "        [0.6249, 0.7514, 0.5071, 0.6078, 0.1853],\n",
      "        [0.3559, 0.7236, 0.1384, 0.3992, 0.3596],\n",
      "        [0.5445, 0.9686, 0.8894, 0.5363, 0.0135]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "sequence = torch.rand(10)*10\n",
    "sequence = sequence.long()\n",
    "fc = FC(vocab_size, embed_dim, fake_pre_embedding, False)\n",
    "print(f\"seqence:{sequence}\")\n",
    "fc(sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
